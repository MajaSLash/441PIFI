{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Context\n",
    "\n",
    "We are using AR face database which is public and access is free. To enable detailed testing and\n",
    "model building the AR face images have been manually labelled with 22 facial features on each\n",
    "face. The 22 points chosen are consistent across all images. This labelled database contains face\n",
    "images of 136 persons (76 men & 60 women). Images feature frontal view faces with different facial\n",
    "expressions and illumination conditions.\n",
    "\n",
    "# Data Format\n",
    "\n",
    "- Male images are stored as: m-xx-yy.pts\n",
    "- Females as: w-xx-yy.pts\n",
    "- 'xx' is a unique person identifier (from \"00\" to \"76\" for men and from \"00\" to \"60\" for\n",
    "women). 'yy' specifies expression or lighting condition. Its meanings are described as\n",
    "follows:\n",
    "\n",
    "```sh\n",
    "1: Neutral expression\n",
    "2: Smile\n",
    "3: Anger\n",
    "5: left light on\n",
    "```\n",
    "\n",
    "# Extract Workload\n",
    "\n",
    "The core focus of the extract workload is to create a flat file representation of all the individuals,\n",
    "which also includes their gender, id, and emotional state, alongside each of the individual points that\n",
    "were gathered from each of their unique facial expresssion image(s). This flat file will be a CSV that will\n",
    "be located within the `ex_res` folder,  where the extracted & minimally preprocessed data. \n",
    "\n",
    "The goal here is to be able to retain as much information as possible, and allow us to craft new features\n",
    "across the whole dataset easily. The CSV will allow us to create a Pandas dataframe which can be easily\n",
    "manipulated into our desired shape(s) when it comes to feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify FaceMarkupARDatabase\n",
    "\n",
    "Ensure the end-user contains our original_dataset and not a manipulated / malformed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The hash of the folder does not match the expected value. Expected: a3b9a9a41f515586fb41411eb6b184dd3d291d7a04f2d3d0a8527181d1ded25a, Actual: ac980862a82d84574ab4037e5dcde757750aa1419e55ec37717c4105493060d8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m folder_hash \u001b[38;5;241m=\u001b[39m hash_folder(folder_path)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m folder_hash \u001b[38;5;241m!=\u001b[39m expected_hash:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe hash of the folder does not match the expected value. Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_hash\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_hash\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFaceMarkupARDatabase has been verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The hash of the folder does not match the expected value. Expected: a3b9a9a41f515586fb41411eb6b184dd3d291d7a04f2d3d0a8527181d1ded25a, Actual: ac980862a82d84574ab4037e5dcde757750aa1419e55ec37717c4105493060d8"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "def hash_file(file_path):\n",
    "    # Generate a hash for a file\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(file_path, 'rb') as file:\n",
    "        while True:\n",
    "            data = file.read(65536)  # Read in 64k chunks\n",
    "            if not data:\n",
    "                break\n",
    "            hasher.update(data)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def hash_folder(folder_path):\n",
    "    # Generate a hash for a folder\n",
    "    folder_hasher = hashlib.sha256()\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            folder_hasher.update(hash_file(file_path).encode('utf-8'))\n",
    "    return folder_hasher.hexdigest()\n",
    "\n",
    "folder_path = \"../../FaceMarkupARDatabase\"\n",
    "expected_hash = \"a3b9a9a41f515586fb41411eb6b184dd3d291d7a04f2d3d0a8527181d1ded25a\"\n",
    "folder_hash = hash_folder(folder_path)\n",
    "\n",
    "if folder_hash != expected_hash:\n",
    "    raise ValueError(f\"The hash of the folder does not match the expected value. Expected: {expected_hash}, Actual: {folder_hash}\")\n",
    "else:\n",
    "    print(\"FaceMarkupARDatabase has been verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Extract Results Folder At Project Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder structure already exists!, Please delete and re-run the extract pipeline if you are running into issues\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_folder_structure():\n",
    "    # Get the root directory of the Git project\n",
    "    root_dir = os.getcwd()\n",
    "\n",
    "    # Define the path for the ex_res folder\n",
    "    ex_res_folder = os.path.join(root_dir, \"ex_res\")\n",
    "\n",
    "    # Check if the folder structure already exists\n",
    "    if not os.path.exists(ex_res_folder):\n",
    "        # If the ex_res folder does not exist, create the folder\n",
    "        os.makedirs(ex_res_folder)\n",
    "        print(\"Folder structure created successfully.\")\n",
    "    else:\n",
    "        print(\"Folder structure already exists!, Please delete and re-run the extract pipeline if you are running into issues\")\n",
    "    \n",
    "    return ex_res_folder\n",
    "\n",
    "ex_res_folder = create_folder_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract & Preprocess\n",
    "\n",
    "Each facial expression from the `.pts` file(s) will be extracted and transformed accordingly:\n",
    "\n",
    "- Gender is a binary column where `0 = Female | 1 = Male`\n",
    "- Emotional Expression is one-hot-encoded, where there are four columns: `['neutral', 'neutral', 'smile', 'anger', 'left_light']`\n",
    "    - There will only be one column with a 1 representing a person's emotional state\n",
    "    - All other columns will be marked with 0\n",
    "- Person's unique ID is a string that combines gender and person_id from the folder holding a person's specific points, for example: `'m' + '001' = 'm001'`\n",
    "\n",
    "The expected result of this workflow should leave us with a file structure like this:\n",
    "\n",
    "```js\n",
    "└───ex_res\n",
    "    └───ex_res.csv\n",
    "```\n",
    "\n",
    "#### Interesting Notes:\n",
    "\n",
    "Women 047-060 only have anger and left_light images, which is a major blow for data that can be used to identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    x: float\n",
    "    y: float\n",
    "    \n",
    "def read_points_from_file(file_name):\n",
    "    points: List[Point] = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            # Remove all trailing whitespaces from line, and if that returns None for the line, skip the line\n",
    "            if line.strip() is None:\n",
    "                continue\n",
    "            \n",
    "            # Ignore all strings that are not points representing facial expression\n",
    "            if line.startswith(('version', 'n_points', '{', '}')):\n",
    "                continue\n",
    "                \n",
    "            x, y = map(float, line.split())\n",
    "            points.append(Point(x, y))\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class EmotionalExpression(Enum):\n",
    "    NEUTRAL = 1\n",
    "    SMILE = 2\n",
    "    ANGER = 3\n",
    "    LEFT_LIGHT = 5\n",
    "\n",
    "def one_hot_encode_emotion(emotion_name):\n",
    "    if emotion_name == 'NEUTRAL':\n",
    "        return [1, 0, 0, 0]\n",
    "    elif emotion_name == 'SMILE':\n",
    "        return [0, 1, 0, 0]\n",
    "    elif emotion_name == 'ANGER':\n",
    "        return [0, 0, 1, 0]\n",
    "    elif emotion_name == 'LEFT_LIGHT':\n",
    "        return [0, 0, 0, 1]\n",
    "    else:\n",
    "        raise ValueError(\"Emotion Name not recognized\")\n",
    "\n",
    "def transform_df_friendly(gender: str, person_id: str, emotional_expr: str):\n",
    "    df_gender = 1 if gender == 'm' else 0\n",
    "    df_person_id = gender + person_id\n",
    "    df_emotional_expr = one_hot_encode_emotion(EmotionalExpression(int(emotional_expr)).name)\n",
    "\n",
    "    return df_gender, df_person_id, df_emotional_expr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "database_path = \"../../FaceMarkupARDatabase/points_22\"\n",
    "\n",
    "num_face_points = 22\n",
    "column_names = ['gender', 'person_id', 'neutral', 'smile', 'anger', 'left_light']\n",
    "\n",
    "for i in range(num_face_points):\n",
    "    column_names.extend([f'p_{i}_x'])\n",
    "    column_names.extend([f'p_{i}_y'])\n",
    "\n",
    "def traverse_facial_expressions():\n",
    "    # Walk through the FaceMarkupARDatabase/points_22 folder\n",
    "    # Create a dataframe that will have gender, person_id, emotional_expr, and 22 points from x and y\n",
    "\n",
    "    df = pd.DataFrame(None, columns=column_names)\n",
    "    for dirpath, _, files in os.walk(database_path):\n",
    "        if 'dummy.pts' in files: # Skip dummy.pts, as its not relevant\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.endswith(\".pts\"):\n",
    "                # Extract Gender & Person Unique ID & emotional_expr\n",
    "                gender, person_id, emotional_expr = file.split('-')\n",
    "                # Expression also contains the suffix of the file extension\n",
    "                # Expression is always two digits that goes from 01, 02, 03, 05\n",
    "                emotional_expr = emotional_expr[:2]\n",
    "                facial_expression_points: List[Point] = read_points_from_file(os.path.join(dirpath, file))\n",
    "                \n",
    "                # Preprocess columns\n",
    "                df_gender, df_person_id, df_emotional_expr_lst = transform_df_friendly(gender, person_id, emotional_expr)\n",
    "                \n",
    "                # Craft Dataframe Row for specific person in specific emotional state\n",
    "                df_row = [df_gender, df_person_id] + df_emotional_expr_lst\n",
    "                \n",
    "                for _, point in enumerate(facial_expression_points):\n",
    "                    df_row.extend([point.x, point.y])\n",
    "                \n",
    "                # Add to flat-file dataframe\n",
    "                df.loc[len(df)] = df_row\n",
    "\n",
    "    return df\n",
    "\n",
    "flat_file_df = traverse_facial_expressions()\n",
    "flat_file_df.to_csv(os.path.join(ex_res_folder, \"ex_res.csv\"), index=True, index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>gender</th>\n",
       "      <th>person_id</th>\n",
       "      <th>neutral</th>\n",
       "      <th>smile</th>\n",
       "      <th>anger</th>\n",
       "      <th>left_light</th>\n",
       "      <th>p_0_x</th>\n",
       "      <th>p_0_y</th>\n",
       "      <th>p_1_x</th>\n",
       "      <th>...</th>\n",
       "      <th>p_17_x</th>\n",
       "      <th>p_17_y</th>\n",
       "      <th>p_18_x</th>\n",
       "      <th>p_18_y</th>\n",
       "      <th>p_19_x</th>\n",
       "      <th>p_19_y</th>\n",
       "      <th>p_20_x</th>\n",
       "      <th>p_20_y</th>\n",
       "      <th>p_21_x</th>\n",
       "      <th>p_21_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>m061</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>337.065</td>\n",
       "      <td>268.794</td>\n",
       "      <td>450.991</td>\n",
       "      <td>...</td>\n",
       "      <td>398.359</td>\n",
       "      <td>366.730</td>\n",
       "      <td>397.026</td>\n",
       "      <td>400.041</td>\n",
       "      <td>398.359</td>\n",
       "      <td>464.666</td>\n",
       "      <td>286.432</td>\n",
       "      <td>378.722</td>\n",
       "      <td>511.618</td>\n",
       "      <td>375.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>m061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327.346</td>\n",
       "      <td>262.923</td>\n",
       "      <td>441.290</td>\n",
       "      <td>...</td>\n",
       "      <td>386.599</td>\n",
       "      <td>374.627</td>\n",
       "      <td>387.108</td>\n",
       "      <td>393.943</td>\n",
       "      <td>389.141</td>\n",
       "      <td>462.566</td>\n",
       "      <td>280.870</td>\n",
       "      <td>383.777</td>\n",
       "      <td>497.921</td>\n",
       "      <td>383.777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>m061</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>325.671</td>\n",
       "      <td>257.013</td>\n",
       "      <td>443.241</td>\n",
       "      <td>...</td>\n",
       "      <td>385.823</td>\n",
       "      <td>364.557</td>\n",
       "      <td>386.734</td>\n",
       "      <td>383.696</td>\n",
       "      <td>386.734</td>\n",
       "      <td>455.696</td>\n",
       "      <td>275.544</td>\n",
       "      <td>379.139</td>\n",
       "      <td>489.722</td>\n",
       "      <td>377.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>m061</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>323.919</td>\n",
       "      <td>295.817</td>\n",
       "      <td>439.650</td>\n",
       "      <td>...</td>\n",
       "      <td>380.946</td>\n",
       "      <td>420.353</td>\n",
       "      <td>380.946</td>\n",
       "      <td>442.577</td>\n",
       "      <td>380.107</td>\n",
       "      <td>497.088</td>\n",
       "      <td>292.471</td>\n",
       "      <td>414.902</td>\n",
       "      <td>485.775</td>\n",
       "      <td>421.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>m066</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>331.813</td>\n",
       "      <td>243.564</td>\n",
       "      <td>455.946</td>\n",
       "      <td>...</td>\n",
       "      <td>390.842</td>\n",
       "      <td>362.180</td>\n",
       "      <td>389.509</td>\n",
       "      <td>375.857</td>\n",
       "      <td>388.926</td>\n",
       "      <td>465.606</td>\n",
       "      <td>262.461</td>\n",
       "      <td>373.525</td>\n",
       "      <td>516.556</td>\n",
       "      <td>377.605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  gender person_id  neutral  smile  anger  left_light    p_0_x  \\\n",
       "0      0       1      m061        0      1      0           0  337.065   \n",
       "1      1       1      m061        1      0      0           0  327.346   \n",
       "2      2       1      m061        0      0      0           1  325.671   \n",
       "3      3       1      m061        0      0      1           0  323.919   \n",
       "4      4       1      m066        1      0      0           0  331.813   \n",
       "\n",
       "     p_0_y    p_1_x  ...   p_17_x   p_17_y   p_18_x   p_18_y   p_19_x  \\\n",
       "0  268.794  450.991  ...  398.359  366.730  397.026  400.041  398.359   \n",
       "1  262.923  441.290  ...  386.599  374.627  387.108  393.943  389.141   \n",
       "2  257.013  443.241  ...  385.823  364.557  386.734  383.696  386.734   \n",
       "3  295.817  439.650  ...  380.946  420.353  380.946  442.577  380.107   \n",
       "4  243.564  455.946  ...  390.842  362.180  389.509  375.857  388.926   \n",
       "\n",
       "    p_19_y   p_20_x   p_20_y   p_21_x   p_21_y  \n",
       "0  464.666  286.432  378.722  511.618  375.391  \n",
       "1  462.566  280.870  383.777  497.921  383.777  \n",
       "2  455.696  275.544  379.139  489.722  377.316  \n",
       "3  497.088  292.471  414.902  485.775  421.192  \n",
       "4  465.606  262.461  373.525  516.556  377.605  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(ex_res_folder, \"ex_res.csv\"))\n",
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
