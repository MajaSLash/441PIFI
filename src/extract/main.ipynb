{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Context\n",
    "\n",
    "We are using AR face database which is public and access is free. To enable detailed testing and\n",
    "model building the AR face images have been manually labelled with 22 facial features on each\n",
    "face. The 22 points chosen are consistent across all images. This labelled database contains face\n",
    "images of 136 persons (76 men & 60 women). Images feature frontal view faces with different facial\n",
    "expressions and illumination conditions.\n",
    "\n",
    "# Data Format\n",
    "\n",
    "- Male images are stored as: m-xx-yy.pts\n",
    "- Females as: w-xx-yy.pts\n",
    "- 'xx' is a unique person identifier (from \"00\" to \"76\" for men and from \"00\" to \"60\" for\n",
    "women). 'yy' specifies expression or lighting condition. Its meanings are described as\n",
    "follows:\n",
    "\n",
    "```sh\n",
    "1: Neutral expression\n",
    "2: Smile\n",
    "3: Anger\n",
    "5: left light on\n",
    "```\n",
    "\n",
    "# Extract Workload\n",
    "\n",
    "The core focus of the extract workload is to create a flat file representation of all the individuals,\n",
    "which also includes their gender, id, and emotional state, alongside each of the individual points that\n",
    "were gathered from each of their unique facial expresssion image(s). This flat file will be a CSV that will\n",
    "be located within the `ex_res` folder,  where the extracted & minimally preprocessed data. \n",
    "\n",
    "The goal here is to be able to retain as much information as possible, and allow us to craft new features\n",
    "across the whole dataset easily. The CSV will allow us to create a Pandas dataframe which can be easily\n",
    "manipulated into our desired shape(s) when it comes to feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify FaceMarkupARDatabase\n",
    "\n",
    "Ensure the end-user contains our original_dataset and not a manipulated / malformed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "def hash_file(file_path):\n",
    "    # Generate a hash for a file\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(file_path, 'rb') as file:\n",
    "        while True:\n",
    "            data = file.read(65536)  # Read in 64k chunks\n",
    "            if not data:\n",
    "                break\n",
    "            hasher.update(data)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def hash_folder(folder_path):\n",
    "    # Generate a hash for a folder\n",
    "    folder_hasher = hashlib.sha256()\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            folder_hasher.update(hash_file(file_path).encode('utf-8'))\n",
    "    return folder_hasher.hexdigest()\n",
    "\n",
    "folder_path = \"../../FaceMarkupARDatabase\"\n",
    "expected_hash = \"a3b9a9a41f515586fb41411eb6b184dd3d291d7a04f2d3d0a8527181d1ded25a\"\n",
    "folder_hash = hash_folder(folder_path)\n",
    "\n",
    "if folder_hash != expected_hash:\n",
    "    raise ValueError(f\"The hash of the folder does not match the expected value. Expected: {expected_hash}, Actual: {folder_hash}\")\n",
    "else:\n",
    "    print(\"FaceMarkupARDatabase has been verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Extract Results Folder At Project Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder structure already exists!, Please delete and re-run the extract pipeline if you are running into issues\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_folder_structure():\n",
    "    # Get the root directory of the Git project\n",
    "    root_dir = os.getcwd()\n",
    "\n",
    "    # Define the path for the ex_res folder\n",
    "    ex_res_folder = os.path.join(root_dir, \"ex_res\")\n",
    "\n",
    "    # Check if the folder structure already exists\n",
    "    if not os.path.exists(ex_res_folder):\n",
    "        # If the ex_res folder does not exist, create the folder\n",
    "        os.makedirs(ex_res_folder)\n",
    "        print(\"Folder structure created successfully.\")\n",
    "    else:\n",
    "        print(\"Folder structure already exists!, Please delete and re-run the extract pipeline if you are running into issues\")\n",
    "    \n",
    "    return ex_res_folder\n",
    "\n",
    "ex_res_folder = create_folder_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract & Preprocess\n",
    "\n",
    "Each facial expression from the `.pts` file(s) will be extracted and transformed accordingly:\n",
    "\n",
    "- Gender is a binary column where `0 = Female | 1 = Male`\n",
    "- Emotional Expression is one-hot-encoded, where there are four columns: `['neutral', 'neutral', 'smile', 'anger', 'left_light']`\n",
    "    - There will only be one column with a 1 representing a person's emotional state\n",
    "    - All other columns will be marked with 0\n",
    "- Person's unique ID is a string that combines gender and person_id from the folder holding a person's specific points, for example: `'m' + '001' = 'm001'`\n",
    "\n",
    "The expected result of this workflow should leave us with a file structure like this:\n",
    "\n",
    "```js\n",
    "└───ex_res\n",
    "    └───ex_res.csv\n",
    "```\n",
    "\n",
    "#### Interesting Notes:\n",
    "\n",
    "Women 047-060 only have anger and left_light images, which is a major blow for data that can be used to identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    x: float\n",
    "    y: float\n",
    "    \n",
    "def read_points_from_file(file_name):\n",
    "    points: List[Point] = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            # Remove all trailing whitespaces from line, and if that returns None for the line, skip the line\n",
    "            if line.strip() is None:\n",
    "                continue\n",
    "            \n",
    "            # Ignore all strings that are not points representing facial expression\n",
    "            if line.startswith(('version', 'n_points', '{', '}')):\n",
    "                continue\n",
    "                \n",
    "            x, y = map(float, line.split())\n",
    "            points.append(Point(x, y))\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class EmotionalExpression(Enum):\n",
    "    NEUTRAL = 1\n",
    "    SMILE = 2\n",
    "    ANGER = 3\n",
    "    LEFT_LIGHT = 5\n",
    "\n",
    "def one_hot_encode_emotion(emotion_name):\n",
    "    if emotion_name == 'NEUTRAL':\n",
    "        return [1, 0, 0, 0]\n",
    "    elif emotion_name == 'SMILE':\n",
    "        return [0, 1, 0, 0]\n",
    "    elif emotion_name == 'ANGER':\n",
    "        return [0, 0, 1, 0]\n",
    "    elif emotion_name == 'LEFT_LIGHT':\n",
    "        return [0, 0, 0, 1]\n",
    "    else:\n",
    "        raise ValueError(\"Emotion Name not recognized\")\n",
    "\n",
    "def transform_df_friendly(gender: str, person_id: str, emotional_expr: str):\n",
    "    df_gender = 1 if gender == 'm' else 0\n",
    "    df_person_id = gender + person_id\n",
    "    df_emotional_expr = one_hot_encode_emotion(EmotionalExpression(int(emotional_expr)).name)\n",
    "\n",
    "    return df_gender, df_person_id, df_emotional_expr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "database_path = \"../../FaceMarkupARDatabase/points_22\"\n",
    "\n",
    "num_face_points = 22\n",
    "column_names = ['gender', 'person_id', 'neutral', 'smile', 'anger', 'left_light']\n",
    "\n",
    "for i in range(num_face_points):\n",
    "    column_names.extend([f'p_{i}_x'])\n",
    "    column_names.extend([f'p_{i}_y'])\n",
    "\n",
    "def traverse_facial_expressions():\n",
    "    # Walk through the FaceMarkupARDatabase/points_22 folder\n",
    "    # Create a dataframe that will have gender, person_id, emotional_expr, and 22 points from x and y\n",
    "\n",
    "    df = pd.DataFrame(None, columns=column_names)\n",
    "    for dirpath, _, files in os.walk(database_path):\n",
    "        if 'dummy.pts' in files: # Skip dummy.pts, as its not relevant\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.endswith(\".pts\"):\n",
    "                # Extract Gender & Person Unique ID & emotional_expr\n",
    "                gender, person_id, emotional_expr = file.split('-')\n",
    "                # Expression also contains the suffix of the file extension\n",
    "                # Expression is always two digits that goes from 01, 02, 03, 05\n",
    "                emotional_expr = emotional_expr[:2]\n",
    "                facial_expression_points: List[Point] = read_points_from_file(os.path.join(dirpath, file))\n",
    "                \n",
    "                # Preprocess columns\n",
    "                df_gender, df_person_id, df_emotional_expr_lst = transform_df_friendly(gender, person_id, emotional_expr)\n",
    "                \n",
    "                # Craft Dataframe Row for specific person in specific emotional state\n",
    "                df_row = [df_gender, df_person_id] + df_emotional_expr_lst\n",
    "                \n",
    "                for _, point in enumerate(facial_expression_points):\n",
    "                    df_row.extend([point.x, point.y])\n",
    "                \n",
    "                # Add to flat-file dataframe\n",
    "                df.loc[len(df)] = df_row\n",
    "\n",
    "    return df\n",
    "\n",
    "flat_file_df = traverse_facial_expressions()\n",
    "flat_file_df.to_csv(os.path.join(ex_res_folder, \"ex_res.csv\"), index=True, index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>gender</th>\n",
       "      <th>person_id</th>\n",
       "      <th>neutral</th>\n",
       "      <th>smile</th>\n",
       "      <th>anger</th>\n",
       "      <th>left_light</th>\n",
       "      <th>p_0_x</th>\n",
       "      <th>p_1_x</th>\n",
       "      <th>p_2_x</th>\n",
       "      <th>...</th>\n",
       "      <th>p_12_y</th>\n",
       "      <th>p_13_y</th>\n",
       "      <th>p_14_y</th>\n",
       "      <th>p_15_y</th>\n",
       "      <th>p_16_y</th>\n",
       "      <th>p_17_y</th>\n",
       "      <th>p_18_y</th>\n",
       "      <th>p_19_y</th>\n",
       "      <th>p_20_y</th>\n",
       "      <th>p_21_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>m001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>328.444</td>\n",
       "      <td>275.496</td>\n",
       "      <td>434.921</td>\n",
       "      <td>...</td>\n",
       "      <td>374.253</td>\n",
       "      <td>395.527</td>\n",
       "      <td>374.253</td>\n",
       "      <td>416.925</td>\n",
       "      <td>373.276</td>\n",
       "      <td>483.314</td>\n",
       "      <td>280.342</td>\n",
       "      <td>404.390</td>\n",
       "      <td>499.835</td>\n",
       "      <td>402.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>m001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344.026</td>\n",
       "      <td>270.090</td>\n",
       "      <td>449.912</td>\n",
       "      <td>...</td>\n",
       "      <td>385.070</td>\n",
       "      <td>389.178</td>\n",
       "      <td>386.006</td>\n",
       "      <td>421.015</td>\n",
       "      <td>390.180</td>\n",
       "      <td>491.438</td>\n",
       "      <td>281.390</td>\n",
       "      <td>393.009</td>\n",
       "      <td>511.685</td>\n",
       "      <td>397.247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>m001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>329.170</td>\n",
       "      <td>291.426</td>\n",
       "      <td>436.553</td>\n",
       "      <td>...</td>\n",
       "      <td>376.119</td>\n",
       "      <td>414.293</td>\n",
       "      <td>378.616</td>\n",
       "      <td>440.265</td>\n",
       "      <td>380.614</td>\n",
       "      <td>505.195</td>\n",
       "      <td>279.723</td>\n",
       "      <td>418.289</td>\n",
       "      <td>501.982</td>\n",
       "      <td>416.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>m001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>345.098</td>\n",
       "      <td>260.392</td>\n",
       "      <td>451.765</td>\n",
       "      <td>...</td>\n",
       "      <td>387.765</td>\n",
       "      <td>367.059</td>\n",
       "      <td>387.765</td>\n",
       "      <td>393.412</td>\n",
       "      <td>387.765</td>\n",
       "      <td>468.706</td>\n",
       "      <td>286.118</td>\n",
       "      <td>383.373</td>\n",
       "      <td>509.490</td>\n",
       "      <td>389.647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>m002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327.193</td>\n",
       "      <td>263.025</td>\n",
       "      <td>437.671</td>\n",
       "      <td>...</td>\n",
       "      <td>386.155</td>\n",
       "      <td>367.268</td>\n",
       "      <td>386.155</td>\n",
       "      <td>394.529</td>\n",
       "      <td>388.628</td>\n",
       "      <td>472.462</td>\n",
       "      <td>282.728</td>\n",
       "      <td>382.655</td>\n",
       "      <td>499.719</td>\n",
       "      <td>377.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>w058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>302.886</td>\n",
       "      <td>287.089</td>\n",
       "      <td>419.544</td>\n",
       "      <td>...</td>\n",
       "      <td>366.684</td>\n",
       "      <td>387.342</td>\n",
       "      <td>373.063</td>\n",
       "      <td>418.329</td>\n",
       "      <td>373.063</td>\n",
       "      <td>483.038</td>\n",
       "      <td>263.696</td>\n",
       "      <td>407.392</td>\n",
       "      <td>467.848</td>\n",
       "      <td>407.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>505</td>\n",
       "      <td>0</td>\n",
       "      <td>w059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>334.510</td>\n",
       "      <td>302.840</td>\n",
       "      <td>450.565</td>\n",
       "      <td>...</td>\n",
       "      <td>392.091</td>\n",
       "      <td>412.199</td>\n",
       "      <td>392.538</td>\n",
       "      <td>433.625</td>\n",
       "      <td>392.538</td>\n",
       "      <td>497.455</td>\n",
       "      <td>285.857</td>\n",
       "      <td>418.448</td>\n",
       "      <td>496.094</td>\n",
       "      <td>425.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>506</td>\n",
       "      <td>0</td>\n",
       "      <td>w059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>327.494</td>\n",
       "      <td>289.823</td>\n",
       "      <td>443.241</td>\n",
       "      <td>...</td>\n",
       "      <td>387.646</td>\n",
       "      <td>382.785</td>\n",
       "      <td>389.468</td>\n",
       "      <td>411.038</td>\n",
       "      <td>389.468</td>\n",
       "      <td>480.304</td>\n",
       "      <td>272.810</td>\n",
       "      <td>405.570</td>\n",
       "      <td>494.278</td>\n",
       "      <td>402.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>507</td>\n",
       "      <td>0</td>\n",
       "      <td>w060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>338.183</td>\n",
       "      <td>300.920</td>\n",
       "      <td>437.272</td>\n",
       "      <td>...</td>\n",
       "      <td>390.743</td>\n",
       "      <td>405.609</td>\n",
       "      <td>391.605</td>\n",
       "      <td>428.442</td>\n",
       "      <td>394.190</td>\n",
       "      <td>482.294</td>\n",
       "      <td>300.702</td>\n",
       "      <td>409.055</td>\n",
       "      <td>473.460</td>\n",
       "      <td>406.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>508</td>\n",
       "      <td>0</td>\n",
       "      <td>w060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>339.342</td>\n",
       "      <td>281.620</td>\n",
       "      <td>439.595</td>\n",
       "      <td>...</td>\n",
       "      <td>394.937</td>\n",
       "      <td>378.228</td>\n",
       "      <td>394.937</td>\n",
       "      <td>403.747</td>\n",
       "      <td>394.937</td>\n",
       "      <td>457.519</td>\n",
       "      <td>301.975</td>\n",
       "      <td>394.633</td>\n",
       "      <td>483.342</td>\n",
       "      <td>384.608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  gender person_id  neutral  smile  anger  left_light    p_0_x  \\\n",
       "0        0       1      m001        1      0      0           0  328.444   \n",
       "1        1       1      m001        0      1      0           0  344.026   \n",
       "2        2       1      m001        0      0      1           0  329.170   \n",
       "3        3       1      m001        0      0      0           1  345.098   \n",
       "4        4       1      m002        1      0      0           0  327.193   \n",
       "..     ...     ...       ...      ...    ...    ...         ...      ...   \n",
       "504    504       0      w058        0      0      0           1  302.886   \n",
       "505    505       0      w059        0      0      1           0  334.510   \n",
       "506    506       0      w059        0      0      0           1  327.494   \n",
       "507    507       0      w060        0      0      1           0  338.183   \n",
       "508    508       0      w060        0      0      0           1  339.342   \n",
       "\n",
       "       p_1_x    p_2_x  ...   p_12_y   p_13_y   p_14_y   p_15_y   p_16_y  \\\n",
       "0    275.496  434.921  ...  374.253  395.527  374.253  416.925  373.276   \n",
       "1    270.090  449.912  ...  385.070  389.178  386.006  421.015  390.180   \n",
       "2    291.426  436.553  ...  376.119  414.293  378.616  440.265  380.614   \n",
       "3    260.392  451.765  ...  387.765  367.059  387.765  393.412  387.765   \n",
       "4    263.025  437.671  ...  386.155  367.268  386.155  394.529  388.628   \n",
       "..       ...      ...  ...      ...      ...      ...      ...      ...   \n",
       "504  287.089  419.544  ...  366.684  387.342  373.063  418.329  373.063   \n",
       "505  302.840  450.565  ...  392.091  412.199  392.538  433.625  392.538   \n",
       "506  289.823  443.241  ...  387.646  382.785  389.468  411.038  389.468   \n",
       "507  300.920  437.272  ...  390.743  405.609  391.605  428.442  394.190   \n",
       "508  281.620  439.595  ...  394.937  378.228  394.937  403.747  394.937   \n",
       "\n",
       "      p_17_y   p_18_y   p_19_y   p_20_y   p_21_y  \n",
       "0    483.314  280.342  404.390  499.835  402.522  \n",
       "1    491.438  281.390  393.009  511.685  397.247  \n",
       "2    505.195  279.723  418.289  501.982  416.291  \n",
       "3    468.706  286.118  383.373  509.490  389.647  \n",
       "4    472.462  282.728  382.655  499.719  377.464  \n",
       "..       ...      ...      ...      ...      ...  \n",
       "504  483.038  263.696  407.392  467.848  407.392  \n",
       "505  497.455  285.857  418.448  496.094  425.590  \n",
       "506  480.304  272.810  405.570  494.278  402.835  \n",
       "507  482.294  300.702  409.055  473.460  406.901  \n",
       "508  457.519  301.975  394.633  483.342  384.608  \n",
       "\n",
       "[509 rows x 51 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(ex_res_folder, \"ex_res.csv\"))\n",
    "test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
